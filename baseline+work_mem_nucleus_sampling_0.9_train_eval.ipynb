{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2,3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kDNQ4ByRWJgk"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eglrKgn5WJgo"
   },
   "outputs": [],
   "source": [
    "examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en', with_info=True,\n",
    "                               as_supervised=True)\n",
    "train_examples, val_examples = examples['train'], examples['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6NKo8tZ_WJgs"
   },
   "outputs": [],
   "source": [
    "tokenizer_path = '../wmt14/tokenizers/pt-en/'\n",
    "dataset = 'ted_pt-en'\n",
    "\n",
    "tokenizer_en = tfds.deprecated.text.SubwordTextEncoder.load_from_file(tokenizer_path+dataset+'_targets_tokenizer')\n",
    "tokenizer_pt = tfds.deprecated.text.SubwordTextEncoder.load_from_file(tokenizer_path+dataset+'_inputs_tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(os.environ[\"CUDA_VISIBLE_DEVICES\"]) > 1:\n",
    "  strategy = tf.distribute.MirroredStrategy()\n",
    "  distributed_flag = True\n",
    "  print ('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "else:\n",
    "  distributed_flag = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NErEV33RWJg0"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 20000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "if distributed_flag:\n",
    "  print('batch size per replica ', BATCH_SIZE // strategy.num_replicas_in_sync)\n",
    "\n",
    "\n",
    "MAX_LENGTH = 40\n",
    "ENC_MEM_SIZE = 0 \n",
    "DEC_MEM_SIZE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bI7QBQ-pWJg2"
   },
   "outputs": [],
   "source": [
    "#change english start and mem\n",
    "def encode(lang1, lang2, mem_size1, mem_size2):\n",
    "  lang1 = [tokenizer_pt.vocab_size+2] * tf.constant(mem_size1).numpy() + [tokenizer_pt.vocab_size] + tokenizer_pt.encode(\n",
    "      lang1.numpy()) + [tokenizer_pt.vocab_size+1]\n",
    " \n",
    "  lang2 = [tokenizer_en.vocab_size+2] * tf.constant(mem_size2).numpy() + [tokenizer_en.vocab_size] + tokenizer_en.encode(\n",
    "      lang2.numpy()) + [tokenizer_en.vocab_size+1]\n",
    "  \n",
    "  return lang1, lang2\n",
    "\n",
    "\n",
    "def tf_encode(pt, en, mem_size_pt, mem_size_en):\n",
    "  result_pt, result_en = tf.py_function(encode, [pt, en, mem_size_pt, mem_size_en], [tf.int64, tf.int64])\n",
    "  result_pt.set_shape([None])\n",
    "  result_en.set_shape([None])\n",
    " \n",
    "  return result_pt, result_en\n",
    "\n",
    "\n",
    "def filter_max_length(x, y, max_length=MAX_LENGTH, mem_size_x=ENC_MEM_SIZE, mem_size_y=DEC_MEM_SIZE):\n",
    "  return tf.logical_and(tf.size(x) - mem_size_x <= max_length,\n",
    "                        tf.size(y) - mem_size_y <= max_length)\n",
    "\n",
    "\n",
    "train_dataset = train_examples.map(lambda x, y: tf_encode(x, y, ENC_MEM_SIZE, DEC_MEM_SIZE))\n",
    "train_dataset = train_dataset.filter(filter_max_length)\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE)\n",
    " \n",
    "val_dataset = val_examples.map(lambda x, y: tf_encode(x, y, ENC_MEM_SIZE, DEC_MEM_SIZE))\n",
    "val_dataset = val_dataset.filter(filter_max_length).padded_batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if distributed_flag:\n",
    "  train_dist_dataset = strategy.experimental_distribute_dataset(train_dataset)\n",
    "  val_dist_dataset = strategy.experimental_distribute_dataset(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 341
    },
    "colab_type": "code",
    "id": "VfpMp-SQWJhD",
    "outputId": "24a28342-574e-40e1-a20f-d837c01dd2dd",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pt_batch, en_batch = next(iter(val_dataset))\n",
    "pt_batch,tokenizer_pt.vocab_size,  en_batch, tokenizer_en.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gvhhA9z0WJhL"
   },
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "  return pos * angle_rates\n",
    "\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "\n",
    "  # apply sin to even indices in the array; 2i\n",
    "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "  # apply cos to odd indices in the array; 2i+1\n",
    "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "  pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "  return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "\n",
    "def create_padding_mask(seq):\n",
    "  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "\n",
    "  # add extra dimensions to add the padding\n",
    "  # to the attention logits.\n",
    "  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "  return mask  # (seq_len, seq_len)\n",
    "\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v, mask, mask_special_tokens=False):\n",
    "  \"\"\"Calculate the attention weights.\n",
    "  q, k, v must have matching leading dimensions.\n",
    "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "  The mask has different shapes depending on its type(padding or look ahead) \n",
    "  but it must be broadcastable for addition.\n",
    "  depth\n",
    "  Args:\n",
    "    q: query shape == (..., seq_len_q, depth) == (batch_size, num_heads, seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v) depthv==depth\n",
    "    mask: Float tensor with shape broadcastable \n",
    "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "\n",
    "  Returns:\n",
    "    output, attention_weights\n",
    "  \"\"\"\n",
    "\n",
    "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "  # scale matmul_qk\n",
    "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "  # add the mask to the scaled tensor.\n",
    "  if mask is not None:\n",
    "    scaled_attention_logits += (mask * -1e9)  \n",
    "\n",
    "  #add mask for start token and first mem token when predicting target sequence\n",
    "  if mask_special_tokens:\n",
    "    b_size = tf.shape(q)[0] \n",
    "    len_q = tf.shape(q)[-2]\n",
    "    len_k = tf.shape(k)[-2]\n",
    "    indices_ax0 = tf.cast(tf.range(DEC_MEM_SIZE +1,len_q),dtype=tf.int64)\n",
    "    #indices to mask start token\n",
    "    indices_ax1 = tf.ones(tf.shape(indices_ax0)[0],dtype=tf.int64) * DEC_MEM_SIZE\n",
    "    #indices to mask first mem token\n",
    "    indices_ax11 = tf.zeros(tf.shape(indices_ax0)[0],dtype=tf.int64)\n",
    "\n",
    "    _mask = tf.sparse.to_dense(tf.sparse.reorder(tf.sparse.SparseTensor(\n",
    "            indices=tf.concat([tf.stack([indices_ax0,indices_ax1],axis=1), \n",
    "                               tf.stack([indices_ax0,indices_ax11],axis=1)],axis=0), #[[i,DEC_MEM_SIZE] for i in range(DEC_MEM_SIZE +1,len_q)],\n",
    "            values=tf.ones(2 * (len_q - DEC_MEM_SIZE - 1)),\n",
    "            dense_shape=[len_q,len_k])))\n",
    "    _mask = _mask[tf.newaxis, :, :]\n",
    "    mask = tf.maximum(tf.zeros((b_size,1,1,len_k)), _mask)\n",
    "\n",
    "    scaled_attention_logits += (mask * -1e9)  \n",
    "\n",
    "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "  # add up to 1.\n",
    "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "  return output, attention_weights\n",
    "\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads):\n",
    "    super(MultiHeadAttention, self).__init__()\n",
    "    self.num_heads = num_heads\n",
    "    self.d_model = d_model\n",
    "\n",
    "    assert d_model % self.num_heads == 0\n",
    "\n",
    "    self.depth = d_model // self.num_heads\n",
    "\n",
    "    self.wq = tf.keras.layers.Dense(d_model)\n",
    "    self.wk = tf.keras.layers.Dense(d_model)\n",
    "    self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "  def split_heads(self, x, batch_size):\n",
    "    \"\"\"\n",
    "    Split the last dimension into (num_heads, depth).\n",
    "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "    \"\"\"\n",
    "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "  def call(self, v, k, q, mask, decoder_masking=False):\n",
    "    batch_size = tf.shape(q)[0]\n",
    "\n",
    "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "    mask_special_tokens = False\n",
    "    if decoder_masking and tf.shape(k)[1] > DEC_MEM_SIZE + 1:\n",
    "      mask_special_tokens = True\n",
    "\n",
    "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "        q, k, v, mask, mask_special_tokens=mask_special_tokens)\n",
    "\n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "    concat_attention = tf.reshape(scaled_attention, \n",
    "                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "    return output, attention_weights\n",
    "\n",
    "\n",
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "  return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "  ])\n",
    "\n",
    "\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "    super(EncoderLayer, self).__init__()\n",
    "\n",
    "    self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "  def call(self, x, training, mask):\n",
    "\n",
    "    attn_output, attn_w = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "    attn_output = self.dropout1(attn_output, training=training)\n",
    "    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "    ffn_output = self.dropout2(ffn_output, training=training)\n",
    "    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "    return out2, attn_w\n",
    "\n",
    "\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads, dff, rate=0.1, masking=False):\n",
    "    super(DecoderLayer, self).__init__()\n",
    "\n",
    "    self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "    self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    self.masking = masking\n",
    "\n",
    "  def call(self, x, enc_output, training, \n",
    "           look_ahead_mask, padding_mask):\n",
    "    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask, \n",
    "                                           decoder_masking=self.masking)  # (batch_size, target_seq_len, d_model)\n",
    "    attn1 = self.dropout1(attn1, training=training)\n",
    "    out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "    attn2, attn_weights_block2 = self.mha2(\n",
    "        enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "    attn2 = self.dropout2(attn2, training=training)\n",
    "    out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "    ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "    ffn_output = self.dropout3(ffn_output, training=training)\n",
    "    out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "    return out3, attn_weights_block1, attn_weights_block2\n",
    "\n",
    "\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "    super(Encoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "\n",
    "    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "    self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
    "                                            self.d_model)\n",
    "\n",
    "\n",
    "    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
    "                       for _ in range(num_layers)]\n",
    "\n",
    "    self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "  def call(self, x, training, mask):\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    attention_weights = {}\n",
    "\n",
    "    # adding embedding and position encoding.\n",
    "    x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "    x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "    x = self.dropout(x, training=training)\n",
    "\n",
    "    for i in range(self.num_layers):\n",
    "      x, attn = self.enc_layers[i](x, training, mask)\n",
    "      attention_weights['encoder_layer{}'.format(i+1)] = attn\n",
    "\n",
    "    return x, attention_weights\n",
    "\n",
    "\n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1, masking=False):\n",
    "    super(Decoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "\n",
    "    self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "    self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "    self.token_type_embedding = tf.keras.layers.Embedding(2, d_model)\n",
    "\n",
    "    self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate, masking) \n",
    "                       for _ in range(num_layers)]\n",
    "    self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "\n",
    "  def call(self, x, enc_output, training, \n",
    "           look_ahead_mask, padding_mask, token_type_inp=None):\n",
    "\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    attention_weights = {}\n",
    "\n",
    "    x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "    x += self.pos_encoding[:, :seq_len, :]\n",
    "    \n",
    "    if token_type_inp is not None:\n",
    "      x += self.token_type_embedding(token_type_inp)\n",
    "    x = self.dropout(x, training=training)\n",
    "\n",
    "    for i in range(self.num_layers):\n",
    "      x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
    "                                             look_ahead_mask, padding_mask)\n",
    "\n",
    "      attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "      attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "\n",
    "    return x, attention_weights\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Transformer(tf.keras.Model):\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
    "               target_vocab_size, pe_input, pe_target, rate=0.1, \n",
    "               masking=False, token_type=False):\n",
    "    super(Transformer, self).__init__()\n",
    "\n",
    "    self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
    "                           input_vocab_size, pe_input, rate)\n",
    "\n",
    "    self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
    "                           target_vocab_size, pe_target, rate, masking)\n",
    "\n",
    "    self.token_type = token_type\n",
    "    if token_type:\n",
    "      self.final_layer = tf.keras.layers.Dense(target_vocab_size+2)\n",
    "      print('token type, ',str(target_vocab_size+2))\n",
    "    else:\n",
    "      self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "      print('no token type, ', str(target_vocab_size))\n",
    "\n",
    "  def call(self, inp, tar, training, enc_padding_mask, \n",
    "           look_ahead_mask, dec_padding_mask, token_type_inp=None):\n",
    "\n",
    "    enc_output, enc_attn = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "\n",
    "    dec_output, attention_weights = self.decoder(\n",
    "        tar, enc_output, training, look_ahead_mask, dec_padding_mask, token_type_inp)\n",
    "\n",
    "    final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size) or target_vocab_size+2\n",
    "    return final_output, enc_attn, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "JcHjndGAWJib",
    "outputId": "c010a009-95fe-4fad-e37f-953c4270aa95"
   },
   "outputs": [],
   "source": [
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "if ENC_MEM_SIZE > 0:\n",
    "  input_vocab_size = tokenizer_pt.vocab_size + 3\n",
    "else:\n",
    "  input_vocab_size = tokenizer_pt.vocab_size + 2\n",
    "if DEC_MEM_SIZE > 0:\n",
    "  target_vocab_size = tokenizer_en.vocab_size + 3\n",
    "else:\n",
    "  target_vocab_size = tokenizer_en.vocab_size + 2\n",
    "  \n",
    "dropout_rate = 0.1\n",
    "\n",
    "masking = False\n",
    "token_type = True\n",
    "\n",
    "print(input_vocab_size, target_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1h3tY9r7WJig",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super(CustomSchedule, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "\n",
    "  def __call__(self, step):\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "\n",
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
    "                                     epsilon=1e-9)\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred, global_batch_num_elems):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "  #loss per example in global batch\n",
    "  return tf.reduce_sum(loss_)/(global_batch_num_elems if distributed_flag else tf.reduce_sum(mask))\n",
    "\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "\n",
    "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
    "                          input_vocab_size, target_vocab_size, \n",
    "                          pe_input=input_vocab_size, \n",
    "                          pe_target=target_vocab_size,\n",
    "                          rate=dropout_rate,\n",
    "                          masking=masking,\n",
    "                          token_type=token_type)\n",
    "\n",
    "\n",
    "def create_masks(inp, tar):\n",
    "  # Encoder padding mask\n",
    "  enc_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "  # Used in the 2nd attention block in the decoder.\n",
    "  # This padding mask is used to mask the encoder outputs.\n",
    "  dec_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "  # Used in the 1st attention block in the decoder.\n",
    "  # It is used to pad and mask future tokens in the input received by \n",
    "  # the decoder.\n",
    "  look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "  dec_target_padding_mask = create_padding_mask(tar)\n",
    "  combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "\n",
    "  return enc_padding_mask, combined_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DqfGLw6kC20B"
   },
   "source": [
    "## Baseline pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEM_TOKENS_NUM = 0\n",
    "NUCLEUS_P = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from https://github.com/ShenakhtPajouh/GPT-language-model-tf.keras/blob/master/utils.py#L126\n",
    "def top_k_sampling(x):\n",
    "  logits = x[0] \n",
    "  k = tf.cast(x[1],dtype=tf.int32) #25, \n",
    "  temperature = x[2] #0.8\n",
    "  #'k must be greater than 0'\n",
    "  values, _ = tf.math.top_k(logits, k=k)\n",
    "  min_value = tf.reduce_min(values)\n",
    "  logits = tf.where(\n",
    "      logits < min_value,\n",
    "      tf.ones_like(logits, dtype=logits.dtype) * -1e9,\n",
    "      logits)\n",
    "  logits = logits / temperature\n",
    "  sample = tf.random.categorical(tf.expand_dims(logits, 0), 1) #tf.multinomial(tf.expand_dims(logits, 0), num_samples=1, output_dtype=tf.int32)\n",
    "  return sample[0] #tf.reduce_sum(sample)\n",
    "\n",
    "def argmax(logits):\n",
    "  return tf.argmax(logits)\n",
    "\n",
    "\n",
    "def nucleus_sampling(x):\n",
    "  logits = x[0] \n",
    "  p = x[1] #0.9\n",
    "  # code from https://github.com/royRLL/Zen-NLG-using-Tensorflow-and-Nucleus-Sampling/blob/main/ProjectCharacterPredictionHP60.ipynb\n",
    "  sortedLogits = tf.sort(logits, direction='DESCENDING')\n",
    "  #Softmax to get the probabilities\n",
    "  sortedProbs = tf.nn.softmax(sortedLogits)\n",
    "  #cumulative sum of the probabilities\n",
    "  probsSum = tf.cumsum(sortedProbs, exclusive=True)\n",
    "  maskedLogits = tf.where(probsSum < p, sortedLogits, tf.ones_like(sortedLogits, dtype=tf.float32)*1e9)  \n",
    "  minLogits= tf.reduce_min(maskedLogits, keepdims=True)  \n",
    "  res_logits = tf.where(\n",
    "      logits < minLogits,\n",
    "      tf.ones_like(logits, dtype=tf.float32) * -1e9,\n",
    "      logits,\n",
    "  )\n",
    "\n",
    "  sample = tf.random.categorical(tf.expand_dims(res_logits, 0), 1) #tf.multinomial(tf.expand_dims(logits, 0), num_samples=1, output_dtype=tf.int32)\n",
    "  return sample[0] #tf.reduce_sum(sample)\n",
    "\n",
    "\n",
    "def sampling(x):\n",
    "  logits = x[0]\n",
    "  temperature = x[1] #0.8\n",
    "  logits = logits / temperature\n",
    "  sample = tf.random.categorical(tf.expand_dims(logits, 0), 1) #tf.multinomial(tf.expand_dims(logits, 0), num_samples=1, output_dtype=tf.int32)\n",
    "  return sample[0] #tf.reduce_sum(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_different_tokens(x):\n",
    "  tar_inp = x[0]\n",
    "  preds = x[1]\n",
    "  token_type = x[2]\n",
    "\n",
    "  def mask_existing_mem_tokens(tar_inp,preds,token_type):\n",
    "    tar_inp_mem = tf.boolean_mask(tar_inp, 1-token_type[:-1])\n",
    "    indices = tf.expand_dims(tf.cast(tar_inp_mem,dtype=tf.int64), 1)\n",
    "    tens = tf.sparse.SparseTensor(\n",
    "      indices=indices,\n",
    "      values=tf.ones(tf.shape(indices)[0]),\n",
    "      dense_shape=[target_vocab_size])\n",
    "    sparse = tf.sparse.reorder(tens)\n",
    "    tokens_mask = tf.sparse.to_dense(sparse)\n",
    "    return preds + (tokens_mask * -1e9)  \n",
    "  \n",
    "  return tf.cond(tf.equal(token_type[-1], 0),\n",
    "                 true_fn=lambda: mask_existing_mem_tokens(tar_inp,preds,token_type),\n",
    "                 false_fn=lambda: preds)\n",
    "\n",
    "\n",
    "def sample_tokens(x):\n",
    "  # tar_inp = x[0]\n",
    "  preds = x[0]\n",
    "  token_type = x[1]\n",
    "  nucleus_p = x[2]\n",
    "  \n",
    "  return tf.cond(tf.equal(token_type, 0),\n",
    "                 true_fn=lambda: nucleus_sampling((tf.squeeze(preds,[0]),nucleus_p)),\n",
    "                 false_fn=lambda: tf.argmax(preds, axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def sample_teacher_forcing(x):\n",
    "  predicted_id_sample = x[0]\n",
    "  token_type_sample = x[1] \n",
    "  tar_inp_sample = x[2] \n",
    "  tar_real_sample = x[3]\n",
    "  #tar_inp starts from the beginning of tar\n",
    "  #token_type sample contains currently predicted token type\n",
    "  ##seq_token_index = tf.math.reduce_sum(token_type_sample[:-1])\n",
    "  #we also take care of current number of seq tokens in token_type_sample:\n",
    "  #  if the number of seq tokens(except start)(which gives index of tar_real element to concatenate)\n",
    "  #  is less than len of tar_real then ok\n",
    "  #  else just return predicted_id_sample\n",
    "\n",
    "  #idx discards mem token types and start token type\n",
    "  idx = tf.math.reduce_sum(token_type_sample[DEC_MEM_SIZE+1:-1])\n",
    "\n",
    "  #if tf.shape(token_type_sample[DEC_MEM_SIZE+1:])[0] - tf.math.reduce_sum(token_type_sample[DEC_MEM_SIZE+1:]) > MEM_TOKENS_NUM:\n",
    "  #token_type_sample = tf.concat([token_type_sample[:-1], tf.constant([1],dtype=tf.int64)],0)\n",
    "  token_type_sample = tf.cond(tf.math.greater(tf.math.subtract(tf.cast(tf.shape(token_type_sample[DEC_MEM_SIZE+1:])[0],\n",
    "                                                                       dtype=tf.int64), \n",
    "                                                               tf.math.reduce_sum(token_type_sample[DEC_MEM_SIZE+1:])),\n",
    "                                              MEM_TOKENS_NUM),\n",
    "                             true_fn=lambda : tf.concat([token_type_sample[:-1], \n",
    "                                                         tf.constant([1],dtype=tf.int64)\n",
    "                                                        ],0),\n",
    "                             false_fn=lambda : token_type_sample)\n",
    "  tar_inp_sample = tf.cond(tf.math.logical_and(tf.equal(token_type_sample[-1], 1), \n",
    "                                               tf.less(tf.cast(idx,dtype=tf.int32), \n",
    "                                                       tf.shape(tar_real_sample)[0])),\n",
    "                          true_fn=lambda : tf.concat([tar_inp_sample, \n",
    "                                                      [tar_real_sample[idx]]\n",
    "                                                     ],0),\n",
    "                          false_fn=lambda : tf.concat([tar_inp_sample, predicted_id_sample],0))\n",
    "  return tar_inp_sample, token_type_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def act_like_body(inp, tar_inp, tar_real, token_type, predictions):\n",
    "  #double predicting previous tokens but teacher forcing saves prev tokens\n",
    "  enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "  predictions, _, _ = transformer(inp, tar_inp, \n",
    "                                  True, \n",
    "                                  enc_padding_mask, \n",
    "                                  combined_mask, \n",
    "                                  dec_padding_mask,\n",
    "                                  token_type_inp=token_type)\n",
    "\n",
    "  token_id_preds = predictions[:,:,:-2]\n",
    "\n",
    "  predicted_token_type = tf.cast(tf.argmax(predictions[: ,-1:, -2:], axis=-1), tf.int64)\n",
    "  token_type = tf.concat([token_type, predicted_token_type], axis=-1)\n",
    "  \n",
    "  #nucleus sampling here\n",
    "  predicted_id = tf.cast(tf.map_fn(sample_tokens, (\n",
    "                                                   token_id_preds[:,-1:,:], \n",
    "                                                   predicted_token_type,\n",
    "                                                   tf.ones(tf.shape(token_id_preds)[0]) * NUCLEUS_P\n",
    "                                                  ),\n",
    "                          fn_output_signature=tf.TensorSpec(shape=(None), dtype=tf.int64)),\n",
    "                         tf.int64)                  \n",
    "  \n",
    "  tar_inp, token_type = tf.map_fn(sample_teacher_forcing,\n",
    "                      (predicted_id, token_type, tar_inp, tar_real), \n",
    "                      fn_output_signature=(tf.TensorSpec(shape=(None), dtype=tf.int64), \n",
    "                                           tf.TensorSpec(shape=(None), dtype=tf.int64)))\n",
    "  \n",
    "  return inp, tar_inp, tar_real, token_type, token_id_preds\n",
    "\n",
    "\n",
    "act_like_condition = lambda inp, tar_inp, tar_real, token_type, predictions: tf.shape(tar_inp)[1] <= tf.shape(tar_real)[1] + MEM_TOKENS_NUM - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_sample_seq_tokens(x):\n",
    "  #here logits and token type are for tokens starting the first after the start token (very first mem tokens and start are discarded)\n",
    "  predicted_logits_sample = x[0]\n",
    "  token_type_sample = x[1]\n",
    "  msg = tf.cond(tf.equal(tf.reduce_sum(token_type_sample), 0),\n",
    "         true_fn=lambda:'zero token type sample',\n",
    "         false_fn=lambda:'ok')\n",
    "\n",
    "  tar_real = x[2]\n",
    "  tar_seq_len = tf.shape(tar_real)[0]\n",
    "  curr_token_logits = tf.boolean_mask(predicted_logits_sample, token_type_sample)[:tar_seq_len,:]\n",
    "  end_token_logits = tf.concat([tf.zeros((tar_seq_len - tf.shape(curr_token_logits)[0],\n",
    "                                         target_vocab_size-DEC_MEM_SIZE-1),\n",
    "                                         dtype=tf.float32),\n",
    "                                tf.ones((tar_seq_len - tf.shape(curr_token_logits)[0], \n",
    "                                         1),\n",
    "                                        dtype=tf.float32),\n",
    "                                tf.zeros((tar_seq_len - tf.shape(curr_token_logits)[0],\n",
    "                                          DEC_MEM_SIZE),\n",
    "                                         dtype=tf.float32)\n",
    "                               ],\n",
    "                               -1)\n",
    "  return tf.concat([curr_token_logits,\n",
    "                    end_token_logits\n",
    "                   ], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sample(i, predicted_logits, token_type, seq_logits, tar_real):\n",
    "  _i = tf.cast(i,dtype=tf.int32)\n",
    "  _tar_seq_len = tf.shape(tar_real)[1]\n",
    "  #token_type sample has no element corresponding to the first token\n",
    "  curr_token_logits = tf.boolean_mask(predicted_logits[_i,:,:], token_type[_i,:])[:_tar_seq_len,:]\n",
    "  padded_token_logits = tf.concat([curr_token_logits,tf.zeros((_tar_seq_len - tf.shape(curr_token_logits)[0],\n",
    "                                                        target_vocab_size),dtype=tf.float32)], 0)\n",
    "\n",
    "  seq_logits = tf.concat([seq_logits,tf.expand_dims(padded_token_logits,0)],0)\n",
    "  i += 1\n",
    "\n",
    "  return i, predicted_logits, token_type, seq_logits, tar_real\n",
    "\n",
    "process_sample_condition = lambda i, predicted_logits, token_type, seq_logits, tar_real: tf.cast(i,dtype=tf.int32) < tf.shape(predicted_logits)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def act_like_train_step(inputs, global_batch_num_elems): #(inp, tar):\n",
    "  inp, tar = inputs\n",
    "    \n",
    "  tar_inp = tar[:, DEC_MEM_SIZE:-1]\n",
    "  tar_real = tar[:, DEC_MEM_SIZE + 1:]\n",
    "  \n",
    " \n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "    #last pass before loss to get logits for the whole sequence\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "    predictions, _, _ = transformer(inp, tar_inp, \n",
    "                                    True, \n",
    "                                    enc_padding_mask, \n",
    "                                    combined_mask, \n",
    "                                    dec_padding_mask,\n",
    "                                    token_type_inp=tf.ones(tf.shape(tar_inp),dtype=tf.int64))\n",
    "\n",
    "    token_id_preds = predictions[:,:,:-2]\n",
    "\n",
    "    \n",
    "    \n",
    "    #discard leading mem tokens from predictions\n",
    "    predictions = token_id_preds[:, DEC_MEM_SIZE:, :]\n",
    "\n",
    "\n",
    "    \n",
    "    loss = loss_function(tar_real, predictions, global_batch_num_elems)\n",
    "\n",
    "  gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "  train_loss(loss)\n",
    "  train_accuracy(tar_real, predictions)\n",
    "  return loss \n",
    "\n",
    "\n",
    "if distributed_flag:\n",
    "  @tf.function(input_signature=[train_dist_dataset.element_spec])\n",
    "  def distributed_train_step(dataset_inputs):\n",
    "    distr_tar = dataset_inputs[1]\n",
    "    concat_tar = tf.concat(distr_tar.values,axis=0)[:, DEC_MEM_SIZE + 1:]\n",
    "    global_batch_num_elems = tf.reduce_sum(tf.cast(tf.math.logical_not(tf.math.equal(concat_tar, 0)),\n",
    "                                                   dtype=tf.float32))\n",
    "\n",
    "    per_replica_losses = strategy.run(act_like_train_step, args=(dataset_inputs,global_batch_num_elems,))\n",
    "    return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,\n",
    "                           axis=None)\n",
    "\n",
    "@tf.function(input_signature=[train_dataset.element_spec])\n",
    "def train_step_single_gpu(dataset_inputs):\n",
    "  return act_like_train_step(dataset_inputs,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "colab_type": "code",
    "id": "gY2UBkDBWJiz",
    "outputId": "01d7d421-d16f-4f92-871a-a47f8c74520f",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "checkpoint_path = './ckpts_baseline+work_mem_nucleus_0.9'\n",
    " \n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                           optimizer=optimizer)\n",
    " \n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=None)\n",
    " \n",
    "if ckpt_manager.latest_checkpoint:\n",
    "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "  print(ckpt_manager.latest_checkpoint)\n",
    "  print('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TEerP83JWJi4"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ENC_MEM_SIZE,DEC_MEM_SIZE,NUCLEUS_P,EPOCHS,MEM_TOKENS_NUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "checkpoint_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "if not os.path.exists(checkpoint_path):\n",
    "    os.makedirs(checkpoint_path)\n",
    "\n",
    "log_file = checkpoint_path+'/log.txt'\n",
    "\n",
    "with open(log_file,'a') as f: \n",
    "  f.write(f\"\\n {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} curr mem tokens num = {MEM_TOKENS_NUM} \\n\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "for epoch in range(EPOCHS):\n",
    "  # TRAIN LOOP\n",
    "  total_loss = 0.0\n",
    "  num_batches = 0\n",
    "  for x in tqdm(train_dist_dataset):\n",
    "    total_loss += distributed_train_step(x)\n",
    "    num_batches += 1\n",
    "    if num_batches % 50 == 0:\n",
    "      \n",
    "      template1 = f\"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} Epoch {epoch+1}, Batch {num_batches}, Loss: {total_loss / num_batches}, distr_elem_loss {train_loss.result()},  Accuracy: {train_accuracy.result()}\\n\"\n",
    "      print(template1)\n",
    "      with open(log_file,'a') as f: \n",
    "        f.write(template1)\n",
    "  \n",
    "  train_loss_averaged = total_loss / num_batches\n",
    " \n",
    "  ckpt_save_path = ckpt_manager.save()\n",
    "  \n",
    "  template = (f\"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} Epoch {epoch+1}, Loss: {train_loss_averaged}, distr_elem_loss {train_loss.result()},  Accuracy: {train_accuracy.result()}, Checkpoint: {ckpt_save_path}\\n\")\n",
    "  print(template)\n",
    "\n",
    "  with open(log_file,'a') as f: \n",
    "    f.write(template)\n",
    "\n",
    "  train_loss.reset_states()\n",
    "  train_accuracy.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "JcHjndGAWJib",
    "outputId": "c010a009-95fe-4fad-e37f-953c4270aa95"
   },
   "outputs": [],
   "source": [
    "masking = False\n",
    "token_type = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEM_TOKENS_NUM = 10\n",
    "NUCLEUS_P = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def act_like_train_step(inputs, global_batch_num_elems):\n",
    "  inp, tar = inputs\n",
    "  tar_inp = tar[:, :DEC_MEM_SIZE+1] #send start token with type 1 (seq) and all the mem tokens with type 0\n",
    "\n",
    "  tar_real = tar[:, DEC_MEM_SIZE + 1:]\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "    #run while loop to obtain predictions logits\n",
    "    init_token_type = tf.concat([tf.zeros(tf.shape(tar_inp[:, :-1]),dtype=tf.int64), \n",
    "                         tf.ones(tf.shape(tar_inp[:, -1:]),dtype=tf.int64)], \n",
    "                        axis=-1)\n",
    "\n",
    "    predictions = tf.zeros((tf.shape(tar_inp)[0], tf.shape(tar_inp)[1], target_vocab_size), dtype=tf.float32)\n",
    "    inp, tar_inp, tar_real, token_type, predictions = tf.while_loop(act_like_condition, act_like_body, \n",
    "                                                      [inp, \n",
    "                                                       tar_inp,\n",
    "                                                       tar_real,\n",
    "                                                       init_token_type,\n",
    "                                                       predictions\n",
    "                                                       ])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #last pass before loss to get logits for the whole sequence\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "    predictions, _, _ = transformer(inp, tar_inp, \n",
    "                                    True, \n",
    "                                    enc_padding_mask, \n",
    "                                    combined_mask, \n",
    "                                    dec_padding_mask,\n",
    "                                    token_type_inp=token_type)\n",
    "\n",
    "    token_id_preds = predictions[:,:,:-2]\n",
    "\n",
    "    predicted_token_type = tf.argmax(predictions[: ,-1:, -2:], axis=-1, output_type=tf.int64)\n",
    "    token_type = tf.concat([token_type, predicted_token_type], axis=-1)\n",
    "\n",
    "    #nucleus sampling here\n",
    "    predicted_id = tf.map_fn(sample_tokens, (token_id_preds[:,-1:,:], \n",
    "                                             predicted_token_type,\n",
    "                                             tf.ones(tf.shape(token_id_preds)[0]) * NUCLEUS_P\n",
    "                                            ),\n",
    "                            fn_output_signature=tf.TensorSpec(shape=(None), dtype=tf.int64))\n",
    "    \n",
    "    tar_inp, token_type = tf.map_fn(sample_teacher_forcing,\n",
    "                        (predicted_id, token_type, tar_inp, tar_real), \n",
    "                        fn_output_signature=(tf.TensorSpec(shape=(None), dtype=tf.int64), \n",
    "                                             tf.TensorSpec(shape=(None), dtype=tf.int64)))\n",
    "\n",
    "    \n",
    "    #discard leading mem tokens from predictions\n",
    "    predictions = token_id_preds[:, DEC_MEM_SIZE:, :]\n",
    "\n",
    "\n",
    "    seq_predictions = tf.map_fn(filter_sample_seq_tokens,\n",
    "                                (predictions, token_type[:,DEC_MEM_SIZE+1:], tar_real), \n",
    "                                fn_output_signature=tf.TensorSpec(shape=(None,None), dtype=tf.float32)\n",
    "                               )\n",
    "    \n",
    "    loss = loss_function(tar_real, seq_predictions, global_batch_num_elems)\n",
    "\n",
    "  gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "  train_loss(loss)\n",
    "  train_accuracy(tar_real, seq_predictions)\n",
    "  return loss \n",
    "\n",
    "if distributed_flag:\n",
    "  @tf.function(input_signature=[train_dist_dataset.element_spec])\n",
    "  def distributed_train_step(dataset_inputs):\n",
    "    distr_tar = dataset_inputs[1]\n",
    "    concat_tar = tf.concat(distr_tar.values,axis=0)[:, DEC_MEM_SIZE + 1:]\n",
    "    global_batch_num_elems = tf.reduce_sum(tf.cast(tf.math.logical_not(tf.math.equal(concat_tar, 0)),\n",
    "                                                   dtype=tf.float32))\n",
    "\n",
    "    per_replica_losses = strategy.run(act_like_train_step, args=(dataset_inputs,global_batch_num_elems,))\n",
    "    return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,\n",
    "                           axis=None)\n",
    "\n",
    "@tf.function(input_signature=[train_dataset.element_spec])\n",
    "def train_step_single_gpu(dataset_inputs):\n",
    "  return act_like_train_step(dataset_inputs,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "colab_type": "code",
    "id": "gY2UBkDBWJiz",
    "outputId": "01d7d421-d16f-4f92-871a-a47f8c74520f",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "checkpoint_path = './ckpts_baseline+work_mem_nucleus_0.9'\n",
    " \n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                           optimizer=optimizer)\n",
    " \n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=None)\n",
    " \n",
    "ckpt.restore(checkpoint_path + '/ckpt-5').assert_existing_objects_matched()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TEerP83JWJi4"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "if not os.path.exists(checkpoint_path):\n",
    "    os.makedirs(checkpoint_path)\n",
    "\n",
    "log_file = checkpoint_path+'/log.txt'\n",
    "\n",
    "with open(log_file,'a') as f: \n",
    "  f.write(f\"\\n {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} curr mem tokens num = {MEM_TOKENS_NUM} \\n\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "for epoch in range(5, EPOCHS):\n",
    "  # TRAIN LOOP\n",
    "  total_loss = 0.0\n",
    "  num_batches = 0\n",
    "  for x in tqdm(train_dist_dataset):\n",
    "    total_loss += distributed_train_step(x)\n",
    "    num_batches += 1\n",
    "    if num_batches % 50 == 0:\n",
    "      \n",
    "      template1 = f\"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} Epoch {epoch+1}, Batch {num_batches}, Loss: {total_loss / num_batches}, distr_elem_loss {train_loss.result()},  Accuracy: {train_accuracy.result()}\\n\"\n",
    "      print(template1)\n",
    "      with open(log_file,'a') as f: \n",
    "        f.write(template1)\n",
    "\n",
    "    \n",
    "  train_loss_averaged = total_loss / num_batches\n",
    "\n",
    "  \n",
    "  ckpt_save_path = ckpt_manager.save()\n",
    "  \n",
    "  template = (f\"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} Epoch {epoch+1}, Loss: {train_loss_averaged}, distr_elem_loss {train_loss.result()},  Accuracy: {train_accuracy.result()}, Checkpoint: {ckpt_save_path}\\n\")\n",
    "  print(template)\n",
    "\n",
    "  with open(log_file,'a') as f: \n",
    "    f.write(template)\n",
    "\n",
    "  train_loss.reset_states()\n",
    "  train_accuracy.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QfcsSWswSdGV"
   },
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(inp_sentence, raw_tokens=False):\n",
    "  start_token = [tokenizer_pt.vocab_size]\n",
    "  end_token = [tokenizer_pt.vocab_size + 1]\n",
    "  \n",
    "  if not raw_tokens:\n",
    "    inp_sentence = start_token + tokenizer_pt.encode(inp_sentence) + end_token\n",
    "    encoder_input = tf.expand_dims(inp_sentence, 0)\n",
    "  \n",
    "  if raw_tokens:\n",
    "    encoder_input = inp_sentence\n",
    "  if len(encoder_input.shape) == 1:\n",
    "    encoder_input = tf.expand_dims(encoder_input, 0)\n",
    "\n",
    "  assert len(encoder_input.shape) == 2  \n",
    "  \n",
    "  if DEC_MEM_SIZE > 0:\n",
    "    decoder_input = [tokenizer_en.vocab_size + 2] * DEC_MEM_SIZE + [tokenizer_en.vocab_size]\n",
    "  else:\n",
    "    decoder_input = [tokenizer_en.vocab_size]\n",
    "\n",
    "  output = tf.expand_dims(decoder_input, 0)\n",
    "  i = len(decoder_input) #i describes the length of currently generated translation\n",
    "  \n",
    "  token_type = tf.concat([tf.zeros(tf.shape(output[:, :-1]),dtype=tf.int64), \n",
    "                           tf.ones(tf.shape(output[:, -1:]),dtype=tf.int64)], \n",
    "                          axis=-1)\n",
    " \n",
    "    \n",
    "  while i <= MAX_LENGTH + DEC_MEM_SIZE + MEM_TOKENS_NUM:\n",
    "    i+=1\n",
    "    \n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, output)    \n",
    "    predictions, enc_att_w, attention_weights = transformer(encoder_input, output, \n",
    "                                                            False, \n",
    "                                                            enc_padding_mask, \n",
    "                                                            combined_mask, \n",
    "                                                            dec_padding_mask,\n",
    "                                                            token_type_inp=token_type)\n",
    "\n",
    "    \n",
    "    token_id_preds = predictions[:,:,:-2]\n",
    "    \n",
    "    predicted_token_type = tf.cast(tf.argmax(predictions[: ,-1:, -2:], axis=-1), tf.int64)\n",
    "    #bound number of generated mem tokens \n",
    "    if tf.cast(tf.shape(token_type[:,DEC_MEM_SIZE+1:])[1],dtype=tf.int64) - tf.math.reduce_sum(token_type[:,DEC_MEM_SIZE+1:]) > MEM_TOKENS_NUM - 1:\n",
    "      predicted_token_type = tf.constant([[1]],dtype=tf.int64)\n",
    "    token_type = tf.concat([token_type, predicted_token_type], axis=-1)\n",
    "    \n",
    "    #nucleus sampling here\n",
    "    predicted_id = tf.cast(tf.map_fn(sample_tokens, (\n",
    "                                                     token_id_preds[:,-1:,:], \n",
    "                                                     predicted_token_type,\n",
    "                                                     tf.ones(tf.shape(token_id_preds)[0]) * NUCLEUS_P\n",
    "                                                    ),\n",
    "                            fn_output_signature=tf.TensorSpec(shape=(None), dtype=tf.int64)),\n",
    "                           tf.int32)                  \n",
    "    \n",
    "    # return the result if the predicted_id is equal to the end token and its type is sequence\n",
    "    if predicted_id == tokenizer_en.vocab_size+1: #and predicted_token_type == 1:\n",
    "      output = tf.concat([output, predicted_id], axis=-1)\n",
    "      final_res = tf.squeeze(output, axis=0)\n",
    "  \n",
    "      if raw_tokens:\n",
    "        predicted_sentence = []\n",
    "        for j,i in enumerate(final_res):\n",
    "          if token_type[0,j] == 1:\n",
    "              predicted_sentence.append(i)\n",
    "        return predicted_sentence, enc_att_w, attention_weights, token_type\n",
    "      else:\n",
    "        return final_res, enc_att_w, attention_weights, token_type\n",
    "      \n",
    "      \n",
    "    output = tf.concat([output, predicted_id], axis=-1)\n",
    "        \n",
    "  final_res = tf.squeeze(output, axis=0)\n",
    "  \n",
    "  if raw_tokens:\n",
    "    predicted_sentence = []\n",
    "    for j,i in enumerate(final_res):\n",
    "      if token_type[0,j] == 1:\n",
    "          predicted_sentence.append(i)\n",
    "    return predicted_sentence, enc_att_w, attention_weights, token_type\n",
    "  else:\n",
    "    return final_res, enc_att_w, attention_weights, token_type\n",
    "\n",
    "\n",
    "def translate(sentence, plot=[]):\n",
    "  result,enc_attn_w, attention_weights, token_type = evaluate(sentence)\n",
    "  predicted_sequence = []\n",
    "  predicted_mem = []\n",
    "  predicted_sentence = []\n",
    "  for j,i in enumerate(result):\n",
    "    if i < tokenizer_en.vocab_size:\n",
    "      if token_type[0,j] == 1:\n",
    "        predicted_sentence.append(tokenizer_en.decode([i]))\n",
    "        predicted_sequence.append(tokenizer_en.decode([i]))\n",
    "      else:\n",
    "        predicted_mem.append(tokenizer_en.decode([i]))\n",
    "        predicted_sequence.append(tokenizer_en.decode([i]))\n",
    "    if i == tokenizer_en.vocab_size:\n",
    "      if token_type[0,j] == 1:\n",
    "        predicted_sentence.append('<start>')\n",
    "        predicted_sequence.append('<start>')\n",
    "      else:\n",
    "        predicted_mem.append('<start>')\n",
    "        predicted_sequence.append('<start>')\n",
    "    if i == tokenizer_en.vocab_size + 1:\n",
    "      if token_type[0,j] == 1:\n",
    "        predicted_sentence.append('<end>')\n",
    "        predicted_sequence.append('<end>')\n",
    "      else:\n",
    "        predicted_mem.append('<end>')\n",
    "        predicted_sequence.append('<end>')\n",
    "    if i == tokenizer_en.vocab_size + 2:\n",
    "      if token_type[0,j] == 1:\n",
    "        predicted_sentence.append('<mem>')\n",
    "        predicted_sequence.append('<mem>') \n",
    "      else:\n",
    "        predicted_mem.append('<mem>')\n",
    "        predicted_sequence.append('<mem>')\n",
    "  if len(plot) == 0:\n",
    "    print('Input: {}'.format(sentence))\n",
    "    print('Predicted sequence: {}'.format(''.join(predicted_sequence)))\n",
    "    print('Predicted memory : ({})'.format(')('.join(predicted_mem)))\n",
    "    print('Predicted translation: {}'.format(''.join(predicted_sentence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in val_examples:\n",
    "  translate(i[0].numpy().decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "\n",
    "def _get_ngrams(segment, max_order):\n",
    "  \"\"\"Extracts all n-grams upto a given maximum order from an input segment.\n",
    "  Args:\n",
    "    segment: text segment from which n-grams will be extracted.\n",
    "    max_order: maximum length in tokens of the n-grams returned by this\n",
    "        methods.\n",
    "  Returns:\n",
    "    The Counter containing all n-grams upto max_order in segment\n",
    "    with a count of how many times each n-gram occurred.\n",
    "  \"\"\"\n",
    "  ngram_counts = collections.Counter()\n",
    "  for order in range(1, max_order + 1):\n",
    "    for i in range(0, len(segment) - order + 1):\n",
    "      ngram = tuple(segment[i:i+order])\n",
    "      ngram_counts[ngram] += 1\n",
    "  return ngram_counts\n",
    "\n",
    "\n",
    "def compute_bleu(reference_corpus, translation_corpus, max_order=4, smooth=False):\n",
    "  \"\"\"Computes BLEU score of translated segments against one or more references.\n",
    "  Args:\n",
    "    reference_corpus: list of lists of references for each translation. Each\n",
    "        reference should be tokenized into a list of tokens.\n",
    "    translation_corpus: list of translations to score. Each translation\n",
    "        should be tokenized into a list of tokens.\n",
    "    max_order: Maximum n-gram order to use when computing BLEU score.\n",
    "    smooth: Whether or not to apply Lin et al. 2004 smoothing.\n",
    "  Returns:\n",
    "    3-Tuple with the BLEU score, n-gram precisions, geometric mean of n-gram\n",
    "    precisions and brevity penalty.\n",
    "  \"\"\"\n",
    "  matches_by_order = [0] * max_order\n",
    "  possible_matches_by_order = [0] * max_order\n",
    "  reference_length = 0\n",
    "  translation_length = 0\n",
    "  for (references, translation) in zip(reference_corpus,\n",
    "                                       translation_corpus):\n",
    "    reference_length += min(len(r) for r in references)\n",
    "    translation_length += len(translation)\n",
    "\n",
    "    merged_ref_ngram_counts = collections.Counter()\n",
    "    for reference in references:\n",
    "      merged_ref_ngram_counts |= _get_ngrams(reference, max_order)\n",
    "    translation_ngram_counts = _get_ngrams(translation, max_order)\n",
    "    overlap = translation_ngram_counts & merged_ref_ngram_counts\n",
    "    for ngram in overlap:\n",
    "      matches_by_order[len(ngram)-1] += overlap[ngram]\n",
    "    for order in range(1, max_order+1):\n",
    "      possible_matches = len(translation) - order + 1\n",
    "      if possible_matches > 0:\n",
    "        possible_matches_by_order[order-1] += possible_matches\n",
    "\n",
    "  precisions = [0] * max_order\n",
    "  for i in range(0, max_order):\n",
    "    if smooth:\n",
    "      precisions[i] = ((matches_by_order[i] + 1.) /\n",
    "                       (possible_matches_by_order[i] + 1.))\n",
    "    else:\n",
    "      if possible_matches_by_order[i] > 0:\n",
    "        precisions[i] = (float(matches_by_order[i]) /\n",
    "                         possible_matches_by_order[i])\n",
    "      else:\n",
    "        precisions[i] = 0.0\n",
    "\n",
    "  if min(precisions) > 0:\n",
    "    p_log_sum = sum((1. / max_order) * math.log(p) for p in precisions)\n",
    "    geo_mean = math.exp(p_log_sum)\n",
    "  else:\n",
    "    geo_mean = 0\n",
    "\n",
    "  ratio = float(translation_length) / reference_length\n",
    "\n",
    "  if ratio > 1.0:\n",
    "    bp = 1.\n",
    "  else:\n",
    "    bp = math.exp(1 - 1. / ratio)\n",
    "\n",
    "  bleu = geo_mean * bp\n",
    "\n",
    "  return (bleu, precisions, bp, ratio, translation_length, reference_length)\n",
    "\n",
    "def explain_bleu(bleu_values):\n",
    "  bleu, precisions, bp, ratio, translation_length, reference_length = bleu_values\n",
    "\n",
    "  print(f\"BLEU score: {bleu:.4}\")\n",
    "  print(\"----------------\")\n",
    "  print(f\"Translated text total length\\t {translation_length}\")\n",
    "  print(f\"Reference text total length\\t {translation_length}\")\n",
    "  print(\"----------------\")\n",
    "  print(f\"n-gram max order was {len(precisions)}\")\n",
    "  print(\"n-gram precisions: \", end=\"\")\n",
    "  for val in precisions:\n",
    "    print(f\"{val:.3}\", end=\" \")\n",
    "  print()\n",
    "  print(f\"Brevity penalty: {bp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val_preprocessed = (val_examples.map(lambda x, y: tf_encode(x, y, ENC_MEM_SIZE, DEC_MEM_SIZE)).filter(filter_max_length))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate('ola')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluate(tf.constant([[2,3,4]]),raw_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "refs = [np.array(ref[DEC_MEM_SIZE:-1]) for (_, ref) in tqdm(val_preprocessed)]\n",
    "trans = [np.array(evaluate(inp, raw_tokens=True)[0][DEC_MEM_SIZE:-1]) for (inp, _) in tqdm(val_preprocessed)]\n",
    "explain_bleu(compute_bleu([[r] for r in refs], trans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "log_file = checkpoint_path+'/log.txt'\n",
    "\n",
    "with open(log_file,'a') as f: \n",
    "  f.write('\\n BLEU {} samples for ckpt # {}\\n'.format(len(refs), 20))\n",
    "\n",
    "orig_stdout = sys.stdout\n",
    "\n",
    "with open(log_file,'a')as sys.stdout:\n",
    "  explain_bleu(compute_bleu([[r] for r in refs], trans))\n",
    "  sys.stdout.close()\n",
    "  sys.stdout=orig_stdout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meteor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detokenize(s):\n",
    "  predicted_sequence = []\n",
    "  predicted_mem = []\n",
    "  predicted_sentence = []\n",
    "  for j,i in enumerate(s):\n",
    "    if i < tokenizer_en.vocab_size:\n",
    "      predicted_sentence.append(tokenizer_en.decode([i]))\n",
    "      predicted_sequence.append(tokenizer_en.decode([i]))\n",
    "    if i == tokenizer_en.vocab_size:\n",
    "      predicted_sentence.append('<start>')\n",
    "      predicted_sequence.append('<start>')\n",
    "    if i == tokenizer_en.vocab_size + 1:\n",
    "      predicted_sentence.append('<end>')\n",
    "      predicted_sequence.append('<end>')\n",
    "    if i == tokenizer_en.vocab_size + 2:\n",
    "      print('mem found')\n",
    "      predicted_sentence.append('<mem>')\n",
    "      predicted_sequence.append('<mem>') \n",
    "  return ''.join(predicted_sentence), ''.join(predicted_sequence), ''.join(predicted_mem)\n",
    "\n",
    "import sys\n",
    "import nltk\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "from nltk.translate.meteor_score import single_meteor_score\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "log_file = checkpoint_path+'/log.txt'\n",
    "\n",
    "val_preprocessed = (val_examples.map(lambda x, y: tf_encode(x, y, ENC_MEM_SIZE, DEC_MEM_SIZE)).filter(filter_max_length))        \n",
    "\n",
    "refs = [np.array(ref[DEC_MEM_SIZE:-1]) for (_, ref) in tqdm(val_preprocessed)]\n",
    "trans = [np.array(evaluate(inp, raw_tokens=True)[0][DEC_MEM_SIZE:-1]) for (inp, _) in tqdm(val_preprocessed)]\n",
    "\n",
    "# Calculate METEOR for each sentence and save the result to a file\n",
    "with open(log_file,'a') as output:\n",
    "    scores = [\n",
    "            single_meteor_score(detokenize(ref)[0], detokenize(pred)[0], alpha=0.9, beta=3, gamma=0.5)\n",
    "            for ref, pred in tqdm(zip(refs, trans))\n",
    "        ]\n",
    "    output.write(f\"\\n {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} METEOR = {np.mean(scores)}, {len(refs)} samples, 10 epochs \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "WtNF3i0KTUb8",
    "RDK7rdR5ZtuN",
    "LNjb1IxjXSIT",
    "AaxL0DXyMcxn"
   ],
   "name": "short_loop_categorical",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "056ed3a869d64beb8ea767d2e02fa51f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "06ac4699dc6b48f48b1cb0742201f470": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4302f8a6a9e54b3a881200fa9c774bdc": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "459d1873fd9841ef9d3b1ca715e90672": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Dl Completed...: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a7356063142e4ff8bdd41b817954ea97",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_056ed3a869d64beb8ea767d2e02fa51f",
      "value": 1
     }
    },
    "45dcbea2acfd4ba18e3fd4b44f56984a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4ebb8a5307fe4702a7ac42f44fd8f063": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_45dcbea2acfd4ba18e3fd4b44f56984a",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c51b33bfa1ef49e28236cadb69ab395c",
      "value": 1
     }
    },
    "5182b706ad9a4650986556a9af6f92f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Extraction completed...: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ddff0e035d7c4e7c91192ffc4f21d183",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5634e197f7fb4ccf8369cec342110549",
      "value": 1
     }
    },
    "5634e197f7fb4ccf8369cec342110549": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "5ed24e590af54e93936d358702334b86": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eec65bbd6f4f476fa963508185a4b72a",
      "placeholder": "​",
      "style": "IPY_MODEL_9207f97334054d0aa21b87667de9a143",
      "value": " 2/2 [11:03&lt;00:00, 331.51s/ file]"
     }
    },
    "6097f12512bc4132b32167351e124897": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b8fea75c546b4e309ab192dcb1076b97",
       "IPY_MODEL_db8d45e30bec48379016b6ba9385a035"
      ],
      "layout": "IPY_MODEL_06ac4699dc6b48f48b1cb0742201f470"
     }
    },
    "8350902f8d8948488c80dbb1c4680b2f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_459d1873fd9841ef9d3b1ca715e90672",
       "IPY_MODEL_ee84bb09ba354ff6822c6159bb0c3091"
      ],
      "layout": "IPY_MODEL_d5b1b9b177f34ea5ba16dfea5341066e"
     }
    },
    "862f9855435d44cfaca1b7bcbe97829f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "86e0285f17564a9d97e689d6139a77b5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "88f1ad28e8fb4bbab384629a7a52202d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9207f97334054d0aa21b87667de9a143": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9449dbb31edf4daebc495b8f02e5ef37": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a7356063142e4ff8bdd41b817954ea97": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a8140ba45f3b4e33910aa78a0139550d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "af54bb723d7244e790cbe3acc4e7ef77": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b8fea75c546b4e309ab192dcb1076b97": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Dl Size...: ",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a8140ba45f3b4e33910aa78a0139550d",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_fbda8877c9cb4f7bac59a012bb1e6a87",
      "value": 1
     }
    },
    "b9e39910638047ec93c19e64dcaee537": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c4090310324947738256162566a81ef7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4ebb8a5307fe4702a7ac42f44fd8f063",
       "IPY_MODEL_d8b31bdedb824e968f86148f6fde927f"
      ],
      "layout": "IPY_MODEL_9449dbb31edf4daebc495b8f02e5ef37"
     }
    },
    "c51b33bfa1ef49e28236cadb69ab395c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "cc4f2036f8e54cf0a1bda6bfe08b7a9a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d5b1b9b177f34ea5ba16dfea5341066e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d8b31bdedb824e968f86148f6fde927f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_88f1ad28e8fb4bbab384629a7a52202d",
      "placeholder": "​",
      "style": "IPY_MODEL_86e0285f17564a9d97e689d6139a77b5",
      "value": " 197195/0 [03:10&lt;00:00, 1028.62 examples/s]"
     }
    },
    "db8d45e30bec48379016b6ba9385a035": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_862f9855435d44cfaca1b7bcbe97829f",
      "placeholder": "​",
      "style": "IPY_MODEL_cc4f2036f8e54cf0a1bda6bfe08b7a9a",
      "value": " 557/? [11:03&lt;00:00,  1.19s/ MiB]"
     }
    },
    "db932206adb94f1d8e252bf834e15765": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5182b706ad9a4650986556a9af6f92f5",
       "IPY_MODEL_5ed24e590af54e93936d358702334b86"
      ],
      "layout": "IPY_MODEL_b9e39910638047ec93c19e64dcaee537"
     }
    },
    "ddff0e035d7c4e7c91192ffc4f21d183": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ee84bb09ba354ff6822c6159bb0c3091": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4302f8a6a9e54b3a881200fa9c774bdc",
      "placeholder": "​",
      "style": "IPY_MODEL_af54bb723d7244e790cbe3acc4e7ef77",
      "value": " 5/5 [11:03&lt;00:00, 132.62s/ url]"
     }
    },
    "eec65bbd6f4f476fa963508185a4b72a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fbda8877c9cb4f7bac59a012bb1e6a87": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
